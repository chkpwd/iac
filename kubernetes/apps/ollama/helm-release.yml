# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/common-3.1.0/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
spec:
  interval: 15m
  chart:
    spec:
      chart: ollama
      version: 1.14.0
      interval: 30m
      sourceRef:
        kind: HelmRepository
        name: ollama-helm
        namespace: flux-system
  driftDetection:
    mode: enabled
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    replicaCount: 1
    image:
      repository: ollama/ollama
      tag: 0.6.6
    ollama:
      port: 11434
      gpu:
        enabled: false
      models:
        pull: [llama3.2:3b, deepseek-r1:7b, llava-llama3]
    extraEnv:
      - name: TZ
        value: "America/New_York"
      - name: OLLAMA_DEBUG
        value: "1"
      # - name: OLLAMA_INTEL_GPU # waiting for igpu support
      #   value: "true"
    ingress:
      enabled: true
      className: int-ingress
      labels:
        external-dns/private: "true"
      hosts:
        - host: "ollama.chkpwd.com"
          paths:
            - path: /
              pathType: Prefix
    persistentVolume:
      enabled: true
      accessMode: ReadWriteOnce
      storageClass: ceph-block
      size: 15Gi
    resources:
      requests:
        cpu: 10m
        memory: 5Gi
        gpu.intel.com/i915: 1
      limits:
        memory: 10Gi
        gpu.intel.com/i915: 1
    livenessProbe:
      enabled: true
      path: /
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
      successThreshold: 1
    readinessProbe:
      enabled: true
      path: /
      initialDelaySeconds: 30
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 6
      successThreshold: 1
    # affinity:
    #   nodeAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       nodeSelectorTerms:
    #         - matchExpressions:
    #             - key: intel.feature.node.kubernetes.io/gpu
    #               operator: In
    #               values:
    #                 - "true"
    # runtimeClassName: nvidia
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 100
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage: 80
    podSecurityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities: {drop: ["ALL"]}
    securityContext:
      runAsNonRoot: false
      fsGroupChangePolicy: OnRootMismatch
      seccompProfile: {type: RuntimeDefault}
    volumes:
      - emptyDir: {}
        name: ollama-temp
    volumeMounts:
      - mountPath: /.ollama
        name: ollama-temp
